"""
Module d'analyse s√©mantique avec K-means clustering et g√©n√©ration de r√©sum√©/carte mentale
"""
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from pydantic import BaseModel, Field
from typing import List, Dict, Any
from pydantic_ai import Agent
import chromadb
from langchain_text_splitters import MarkdownTextSplitter
import json
from pathlib import Path
import logging

# Configuration du logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# ============================================
# MOD√àLES PYDANTIC POUR LE R√âSUM√â
# ============================================

class KeyConcept(BaseModel):
    """Un concept cl√© extrait du document"""
    name: str = Field(description="Nom du concept")
    description: str = Field(description="Description br√®ve du concept")
    importance: int = Field(description="Importance de 1 √† 5", ge=1, le=5)


class DocumentSummary(BaseModel):
    """R√©sum√© structur√© d'un document"""
    title: str = Field(description="Titre ou sujet principal du document")
    summary: str = Field(description="R√©sum√© concis en 2-3 phrases")
    key_concepts: List[KeyConcept] = Field(description="Concepts cl√©s identifi√©s (3-5 concepts)")
    main_themes: List[str] = Field(description="Th√®mes principaux abord√©s (3-5 th√®mes)")
    document_type: str = Field(description="Type de document (rapport, proc√®s-verbal, circulaire, etc.)")


# ============================================
# MOD√àLES PYDANTIC POUR LA CARTE MENTALE
# ============================================

class MindMapNode(BaseModel):
    """Noeud de la carte mentale"""
    title: str = Field(description="Titre du noeud")
    children: List["MindMapNode"] = Field(default_factory=list, description="Sous-noeuds (2-4 enfants max)")


class MindMap(BaseModel):
    """Carte mentale compl√®te"""
    central_theme: str = Field(description="Th√®me central de la carte mentale")
    branches: List[MindMapNode] = Field(description="Branches principales de la carte (3-5 branches)")


class SemanticAnalyzer:
    """Analyseur s√©mantique avec clustering et g√©n√©ration de r√©sum√©/mindmap"""
    
    def __init__(self, chroma_db_path: str = "./chromadb_data"):
        """
        Initialise l'analyseur s√©mantique
        
        Args:
            chroma_db_path: Chemin vers la base ChromaDB
        """
        self.client = chromadb.PersistentClient(path=chroma_db_path)
        self.collection = self.client.get_or_create_collection(name="ressources")
        self.splitter = MarkdownTextSplitter()
        
        # Agents Pydantic AI
        self.summary_agent = Agent(
            "mistral:mistral-large-latest",
            output_type=DocumentSummary,
            system_prompt="""Tu es un assistant expert en analyse documentaire fran√ßaise.
Tu dois analyser des extraits de documents et produire un r√©sum√© structur√©.
Identifie le type de document, les th√®mes principaux et les concepts cl√©s.
R√©ponds toujours en fran√ßais avec pr√©cision et concision."""
        )
        
        self.mindmap_agent = Agent(
            "mistral:mistral-large-latest",
            output_type=MindMap,
            system_prompt="""Tu es un expert en organisation de l'information et cr√©ation de cartes mentales.
Tu dois analyser des documents et cr√©er une carte mentale hi√©rarchique.
Organise l'information de mani√®re logique avec un th√®me central et des branches.
Chaque branche peut avoir des sous-branches (2-3 niveaux max).
R√©ponds toujours en fran√ßais."""
        )
    
    def add_document_to_collection(self, markdown_content: str, document_id: str, metadata: Dict[str, Any]):
        """
        Ajoute un document markdown √† la collection ChromaDB en le d√©coupant en chunks
        
        Args:
            markdown_content: Contenu markdown du document
            document_id: Identifiant unique du document
            metadata: M√©tadonn√©es du document
        """
        logger.info(f"D√©coupage du document {document_id} en chunks...")
        chunks = self.splitter.split_text(markdown_content)
        logger.info(f"Document d√©coup√© en {len(chunks)} chunks")
        
        for i, chunk in enumerate(chunks):
            chunk_metadata = {
                **metadata,
                "chunk_index": i
            }
            self.collection.add(
                documents=[chunk],
                metadatas=[chunk_metadata],
                ids=[f"{document_id}_chunk_{i}"]
            )
        
        logger.info(f"‚úì Document {document_id} ajout√© √† la collection")
    
    def find_optimal_clusters(self, embeddings: np.ndarray, max_k: int = 15) -> int:
        """
        Trouve le nombre optimal de clusters avec la m√©thode silhouette
        
        Args:
            embeddings: Embeddings des chunks
            max_k: Nombre maximum de clusters √† tester
            
        Returns:
            Nombre optimal de clusters
        """
        K_range = range(2, min(max_k, len(embeddings)))
        silhouette_scores = []
        
        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(embeddings)
            silhouette_scores.append(silhouette_score(embeddings, kmeans.labels_))
        
        optimal_k = K_range[np.argmax(silhouette_scores)]
        return optimal_k
    
    def get_representative_chunks(self, document_id: str, n_per_cluster: int = 2):
        """
        R√©cup√®re les chunks repr√©sentatifs d'un document via clustering K-means
        
        Args:
            document_id: Identifiant du document
            n_per_cluster: Nombre de chunks √† s√©lectionner par cluster
            
        Returns:
            Tuple (chunks repr√©sentatifs, m√©triques de couverture)
        """
        logger.info(f"R√©cup√©ration des chunks pour {document_id}...")
        # R√©cup√©rer tous les chunks du document avec embeddings
        results = self.collection.get(
            where={"document_id": document_id},
            include=["embeddings", "documents", "metadatas"]
        )
        logger.info(f"Trouv√© {len(results['ids']) if results['ids'] else 0} chunks")
        
        if not results['ids'] or len(results['ids']) < 2:
            # Pas assez de chunks pour le clustering
            metrics = {
                "coverage_percentage": 100.0,
                "n_clusters": 1,
                "total_chunks": len(results['documents']) if results['documents'] else 0,
                "representative_chunks": len(results['documents']) if results['documents'] else 0,
                "avg_similarity": 1.0
            }
            return results['documents'], metrics
        
        embeddings = np.array(results['embeddings'])
        documents = results['documents']
        metadatas = results['metadatas']
        ids = results['ids']
        
        # Trouver le nombre optimal de clusters
        logger.info("Recherche du nombre optimal de clusters...")
        n_clusters = self.find_optimal_clusters(embeddings)
        logger.info(f"Nombre optimal de clusters: {n_clusters}")
        
        # Appliquer K-Means
        logger.info("Application du K-Means clustering...")
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings)
        logger.info("‚úì Clustering termin√©")
        
        # Organiser par cluster
        clusters = {}
        for idx, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append({
                'id': ids[idx],
                'document': documents[idx],
                'metadata': metadatas[idx],
                'embedding': embeddings[idx]
            })
        
        # S√©lectionner les chunks repr√©sentatifs (plus proches du centro√Øde)
        representative_chunks = []
        for cluster_id, items in clusters.items():
            centroid = kmeans.cluster_centers_[cluster_id]
            distances = [(np.linalg.norm(item['embedding'] - centroid), item) for item in items]
            distances.sort(key=lambda x: x[0])
            
            for dist, item in distances[:n_per_cluster]:
                representative_chunks.append(item)
        
        # Calculer la couverture s√©mantique
        selected_embeddings = np.array([chunk['embedding'] for chunk in representative_chunks])
        similarity_matrix = cosine_similarity(embeddings, selected_embeddings)
        max_similarities = similarity_matrix.max(axis=1)
        coverage_percentage = 100 * np.sum(max_similarities >= 0.7) / len(embeddings)
        
        metrics = {
            "coverage_percentage": float(coverage_percentage),
            "n_clusters": int(n_clusters),
            "total_chunks": len(documents),
            "representative_chunks": len(representative_chunks),
            "avg_similarity": float(np.mean(max_similarities))
        }
        
        return [chunk['document'] for chunk in representative_chunks], metrics
    
    async def generate_summary(self, context: str, metrics: Dict[str, Any]) -> DocumentSummary:
        """
        G√©n√®re un r√©sum√© structur√© avec Pydantic AI
        
        Args:
            context: Contexte textuel des chunks repr√©sentatifs
            metrics: M√©triques de couverture s√©mantique
            
        Returns:
            R√©sum√© structur√©
        """
        logger.info("G√©n√©ration du r√©sum√© avec Pydantic AI...")
        prompt = f"""Analyse les extraits de documents suivants et g√©n√®re un r√©sum√© structur√©.

üìä Ces extraits repr√©sentent {metrics['coverage_percentage']:.1f}% de l'information s√©mantique totale
   ({metrics['representative_chunks']} chunks repr√©sentatifs sur {metrics['total_chunks']} au total).

=== EXTRAITS DES DOCUMENTS ===
{context}
=== FIN DES EXTRAITS ===

G√©n√®re un r√©sum√© complet avec:
- Un titre descriptif
- Un r√©sum√© en 2-3 phrases
- Les concepts cl√©s (3-5)
- Les th√®mes principaux (3-5)
- Le type de document"""

        try:
            result = await self.summary_agent.run(prompt)
            logger.info("‚úì R√©sum√© g√©n√©r√© avec succ√®s")
            return result.output
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du r√©sum√©: {str(e)}")
            raise
    
    async def generate_mindmap(self, context: str, summary: DocumentSummary) -> MindMap:
        """
        G√©n√®re une carte mentale avec Pydantic AI
        
        Args:
            context: Contexte textuel des chunks repr√©sentatifs
            summary: R√©sum√© du document
            
        Returns:
            Carte mentale structur√©e
        """
        logger.info("G√©n√©ration de la carte mentale avec Pydantic AI...")
        themes_str = ", ".join(summary.main_themes)
        concepts_str = ", ".join([c.name for c in summary.key_concepts])
        
        prompt = f"""Cr√©e une carte mentale hi√©rarchique bas√©e sur les documents suivants.

üìå Contexte du r√©sum√©:
- Titre: {summary.title}
- Type: {summary.document_type}
- Th√®mes identifi√©s: {themes_str}
- Concepts cl√©s: {concepts_str}

=== EXTRAITS DES DOCUMENTS ===
{context}
=== FIN DES EXTRAITS ===

G√©n√®re une carte mentale avec:
- Un th√®me central repr√©sentatif
- 3 √† 5 branches principales
- Chaque branche peut avoir 2-4 sous-branches
- Maximum 3 niveaux de profondeur"""

        try:
            result = await self.mindmap_agent.run(prompt)
            logger.info("‚úì Carte mentale g√©n√©r√©e avec succ√®s")
            return result.output
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de la carte mentale: {str(e)}")
            raise
    
    def generate_mermaid_mindmap(self, mind_map: MindMap) -> str:
        """
        G√©n√®re le code Mermaid pour la carte mentale
        
        Args:
            mind_map: Carte mentale structur√©e
            
        Returns:
            Code Mermaid
        """
        lines = ["mindmap", f"  root(({mind_map.central_theme}))"]
        
        def add_node(node: MindMapNode, level: int = 2):
            indent = "  " * level
            title = node.title.replace("(", "[").replace(")", "]")
            lines.append(f"{indent}{title}")
            for child in node.children:
                add_node(child, level + 1)
        
        for branch in mind_map.branches:
            add_node(branch)
        
        return "\n".join(lines)
    
    async def analyze_document(self, markdown_files: List[str], document_id: str, output_dir: Path):
        """
        Analyse compl√®te d'un document: clustering, r√©sum√© et carte mentale
        
        Args:
            markdown_files: Liste des fichiers markdown √† analyser
            document_id: Identifiant du document
            output_dir: Dossier de sortie pour les r√©sultats
            
        Returns:
            Dict avec les r√©sultats de l'analyse
        """
        logger.info(f"üöÄ D√©but de l'analyse s√©mantique pour {document_id}")
        logger.info(f"Nombre de fichiers markdown: {len(markdown_files)}")
        
        try:
            # Ajouter tous les fichiers markdown √† la collection
            logger.info("üìÑ Ajout des documents √† la collection ChromaDB...")
            for i, md_file in enumerate(markdown_files, 1):
                logger.info(f"  Traitement du fichier {i}/{len(markdown_files)}: {Path(md_file).name}")
                with open(md_file, "r", encoding="utf-8") as f:
                    content = f.read()
                    metadata = {
                        "document_id": document_id,
                        "file_path": str(md_file)
                    }
                    self.add_document_to_collection(content, document_id, metadata)
            
            # R√©cup√©rer les chunks repr√©sentatifs
            logger.info("üîç Extraction des chunks repr√©sentatifs...")
            representative_chunks, metrics = self.get_representative_chunks(document_id)
            logger.info(f"‚úì {len(representative_chunks)} chunks repr√©sentatifs extraits")
            
            # Pr√©parer le contexte
            logger.info("üìù Pr√©paration du contexte...")
            context = "\n---\n".join(representative_chunks)
            logger.info(f"Contexte pr√©par√©: {len(context)} caract√®res")
            
            # G√©n√©rer le r√©sum√©
            logger.info("ü§ñ G√©n√©ration du r√©sum√© IA...")
            summary = await self.generate_summary(context, metrics)
            
            # G√©n√©rer la carte mentale
            logger.info("üó∫Ô∏è G√©n√©ration de la carte mentale IA...")
            mindmap = await self.generate_mindmap(context, summary)
            
            # G√©n√©rer le code Mermaid
            logger.info("üìä G√©n√©ration du code Mermaid...")
            mermaid_code = self.generate_mermaid_mindmap(mindmap)
            logger.info("‚úì Code Mermaid g√©n√©r√©")
        
            # Sauvegarder les r√©sultats
            logger.info("üíæ Sauvegarde des r√©sultats...")
            analysis_result = {
                "metadata": {
                    "document_id": document_id,
                    "source_files": [str(md_file) for md_file in markdown_files],
                    **metrics
                },
                "summary": summary.model_dump(),
                "mind_map": mindmap.model_dump()
            }
            
            # Sauvegarder le JSON
            json_path = output_dir / "analysis_result.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            logger.info(f"‚úì R√©sultat JSON sauvegard√©: {json_path}")
            
            # Sauvegarder la carte mentale Mermaid
            mindmap_dir = output_dir / "mind_map"
            mindmap_dir.mkdir(parents=True, exist_ok=True)
            mindmap_path = mindmap_dir / "mind_map.md"
            with open(mindmap_path, "w", encoding="utf-8") as f:
                f.write(f"# Carte Mentale - {summary.title}\n\n")
                f.write("```mermaid\n")
                f.write(mermaid_code)
                f.write("\n```\n")
            logger.info(f"‚úì Carte mentale sauvegard√©e: {mindmap_path}")
            
            logger.info(f"üéâ Analyse s√©mantique termin√©e avec succ√®s pour {document_id}")
            
            return {
                "summary": summary.model_dump(),
                "mindmap": mindmap.model_dump(),
                "metrics": metrics,
                "files": {
                    "analysis_json": str(json_path),
                    "mindmap_md": str(mindmap_path)
                }
            }
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse s√©mantique de {document_id}: {str(e)}")
            logger.exception("Traceback complet:")
            raise
